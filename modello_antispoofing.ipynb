{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUWDOuz8TNfn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import models\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torchinfo import summary\n",
        "from torch.cuda.amp import GradScaler\n",
        "\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5O4LiglUNLE",
        "outputId": "04b3d3a0-d653-4fcc-d755-194b0a2b2b1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Definizione del Dataset e Caricamento dei Dati\n",
        "\n",
        "In questa sezione, prepariamo il nostro dataset per l'addestramento e la valutazione del modello. Le operazioni principali sono:\n",
        "\n",
        "1.  **Configurazione Iniziale**: Vengono definiti i percorsi (`root_color_train`, `root_depth_test`, ecc.) alle directory contenenti le immagini a colori e le mappe di profonditÃ . Vengono inoltre impostati gli iperparametri fondamentali come la dimensione delle immagini (`img_size`), la grandezza dei batch (`batch_size`) e i parametri per l'ottimizzatore (`lr`, `weight_decay`).\n",
        "\n",
        "2.  **Creazione della Classe `AntiSpoofDataset`**: PoichÃ© il nostro modello utilizza un input multimodale (**immagini RGB** e **mappe di profonditÃ **), creiamo una classe `Dataset` personalizzata. Questa classe gestisce:\n",
        "    *   Il caricamento simultaneo di un'immagine a colori e della sua corrispondente mappa di profonditÃ .\n",
        "    *   Il ridimensionamento di entrambe le immagini a una dimensione uniforme (`img_size`).\n",
        "    *   L'applicazione di trasformazioni e data augmentation (definite in una cella successiva).\n",
        "    *   La conversione delle immagini in tensori PyTorch pronti per essere inviati al modello.\n",
        "\n",
        "3.  **Funzione `load_paths` e Caricamento**: La funzione `load_paths` esamina le directory specificate, identifica i campioni come `real` (etichetta 0) o `fake` (etichetta 1) in base al nome del file e crea le liste di percorsi e etichette. Infine, questa funzione viene eseguita per caricare i dati di training e test, stampando un riepilogo del numero di campioni per ogni set e per ogni classe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "eDJ6gxPgUZkl"
      },
      "outputs": [],
      "source": [
        "root_color_train = \"/content/drive/MyDrive/fr_gans/dataset_casia/train_img/color\"\n",
        "root_depth_train = \"/content/drive/MyDrive/fr_gans/dataset_casia/train_img/depth_midas_train\"\n",
        "\n",
        "root_color_test = \"/content/drive/MyDrive/fr_gans/dataset_casia/test_img/color\"\n",
        "root_depth_test = \"/content/drive/MyDrive/fr_gans/dataset_casia/test_img/depth_midas_test\"\n",
        "\n",
        "img_size = 224\n",
        "batch_size = 24\n",
        "lr = 1e-4\n",
        "weight_decay = 5e-4\n",
        "patience = 15\n",
        "use_mixed_precision = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPsbwVR6V0au",
        "outputId": "93eddcab-2dff-48b0-ee06-e2921e4a95dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Caricati 2408 campioni da /content/drive/MyDrive/fr_gans/dataset_casia/train_img/color\n",
            "   Real: 591 | Fake: 1817\n",
            "âœ… Caricati 1655 campioni da /content/drive/MyDrive/fr_gans/dataset_casia/test_img/color\n",
            "   Real: 404 | Fake: 1251\n",
            "\n",
            "ðŸ“Š Train: 2408 | Test: 1655\n"
          ]
        }
      ],
      "source": [
        "class AntiSpoofDataset(Dataset):\n",
        "    def __init__(self, color_paths, depth_paths, labels, transform=None):\n",
        "        self.color_paths = color_paths\n",
        "        self.depth_paths = depth_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.color_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Carica immagini\n",
        "        img = cv2.imread(self.color_paths[idx])\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        depth = cv2.imread(self.depth_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "        # Resize\n",
        "        img = cv2.resize(img, (img_size, img_size))\n",
        "        depth = cv2.resize(depth, (img_size, img_size))\n",
        "\n",
        "        if self.transform:\n",
        "            # Augmentation\n",
        "            augmented = self.transform(image=img, mask=depth)\n",
        "            img = augmented[\"image\"]    # Tensor [3, H, W]\n",
        "            depth = augmented[\"mask\"]   # Numpy [H, W] uint8 [0-255]\n",
        "\n",
        "            # Converti NUMPY â†’ Tensor normalizzato\n",
        "            if isinstance(depth, np.ndarray):\n",
        "                depth = torch.from_numpy(depth).float() / 255.0\n",
        "            else:\n",
        "                # Se albumentations giÃ  converte (versioni diverse)\n",
        "                depth = depth.float() / 255.0\n",
        "\n",
        "            depth = depth.unsqueeze(0)  # [1, H, W]\n",
        "        else:\n",
        "            # Senza augmentation\n",
        "            img = torch.from_numpy(img.transpose(2, 0, 1)).float() / 255.0\n",
        "            depth = torch.from_numpy(depth).float() / 255.0\n",
        "            depth = depth.unsqueeze(0)\n",
        "\n",
        "        return img, depth, label\n",
        "\n",
        "\n",
        "def load_paths(root_color, root_depth):\n",
        "    color_paths, depth_paths, labels = [], [], []\n",
        "\n",
        "    for f in os.listdir(root_color):\n",
        "        if not f.lower().endswith((\".jpg\", \".png\")):\n",
        "            continue\n",
        "        f_lower = f.lower()\n",
        "\n",
        "        if \"real\" in f_lower:\n",
        "            label = 0\n",
        "        elif \"fake\" in f_lower:\n",
        "            label = 1\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        color_path = os.path.join(root_color, f)\n",
        "        depth_path = os.path.join(root_depth, f)\n",
        "\n",
        "        if os.path.exists(depth_path):\n",
        "            color_paths.append(color_path)\n",
        "            depth_paths.append(depth_path)\n",
        "            labels.append(label)\n",
        "\n",
        "    print(f\"âœ… Caricati {len(color_paths)} campioni da {root_color}\")\n",
        "    print(f\"   Real: {labels.count(0)} | Fake: {labels.count(1)}\")\n",
        "    return color_paths, depth_paths, labels\n",
        "\n",
        "\n",
        "train_color, train_depth, train_labels = load_paths(root_color_train, root_depth_train)\n",
        "test_color, test_depth, test_labels = load_paths(root_color_test, root_depth_test)\n",
        "\n",
        "print(f\"\\nTrain: {len(train_color)} | Test: {len(test_color)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Augmentation e Creazione dei DataLoader\n",
        "\n",
        "Questa cella si occupa di due compiti fondamentali: definire le strategie di data augmentation e creare i `DataLoader` di PyTorch, che preparano i dati in lotti (batch) per l'addestramento e la valutazione.\n",
        "\n",
        "#### Data Augmentation con Albumentations\n",
        "Per rendere il modello piÃ¹ robusto e capace di generalizzare a condizioni del mondo reale, viene applicata una pipeline di **data augmentation** molto ricca al solo set di addestramento, utilizzando la libreria `Albumentations`. Le trasformazioni sono state scelte per simulare scenari di attacco comuni:\n",
        "\n",
        "-   **Variazioni di Illuminazione**: `RandomBrightnessContrast`, `RandomGamma` e `HueSaturationValue` per simulare diverse condizioni di luce ambientale.\n",
        "-   **Artefatti di Display/Stampa**: `GaussNoise`, `ISONoise`, `MotionBlur` e `GaussianBlur` per imitare il rumore e le sfocature introdotte da schermi, proiettori o stampe di bassa qualitÃ .\n",
        "-   **Diverse Angolazioni e Prospettive**: `ShiftScaleRotate` e `Perspective` per simulare riprese del volto da diverse angolazioni.\n",
        "-   **QualitÃ  del Sensore**: `CLAHE` e `Sharpen` per simulare l'effetto di diverse fotocamere.\n",
        "\n",
        "L'argomento `additional_targets={'mask': 'mask'}` Ã¨ cruciale: garantisce che le trasformazioni geometriche (come rotazioni o ritagli) vengano applicate in modo identico sia all'immagine RGB sia alla mappa di profonditÃ , mantenendole perfettamente allineate.\n",
        "\n",
        "Il set di test, invece, subisce solo la normalizzazione e la conversione in tensore per garantire una valutazione oggettiva e riproducibile delle performance del modello.\n",
        "\n",
        "#### Gestione dello Sbilanciamento delle Classi\n",
        "Come osservato in precedenza, il dataset Ã¨ sbilanciato, con un numero maggiore di campioni \"fake\". Per evitare che il modello diventi \"pigro\" e prediliga la classe maggioritaria, viene utilizzato un `WeightedRandomSampler`. Questo campionatore assegna un peso maggiore ai campioni della classe meno rappresentata (in questo caso, \"real\"), assicurando che durante l'addestramento il modello veda un numero piÃ¹ bilanciato di esempi per ogni classe.\n",
        "\n",
        "#### Creazione dei DataLoader\n",
        "Infine, vengono creati i `DataLoader` per i set di training e test. Questi oggetti gestiscono il caricamento efficiente dei dati in batch, sfruttando il campionatore pesato per il training e mantenendo l'ordine sequenziale per il testing (`shuffle=False`). L'opzione `pin_memory=True` viene usata per velocizzare il trasferimento dei dati sulla GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "CDiZoDgsZo9g"
      },
      "outputs": [],
      "source": [
        "train_tf = A.Compose([\n",
        "    # Simula diverse condizioni di illuminazione\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.4, p=0.7),\n",
        "    A.RandomGamma(gamma_limit=(70, 130), p=0.5),\n",
        "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.6),\n",
        "\n",
        "    # Simula artefatti di stampa/display\n",
        "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.4),\n",
        "    A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=0.3),\n",
        "    A.MotionBlur(blur_limit=7, p=0.4),\n",
        "    A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
        "\n",
        "    # Simula diverse angolazioni\n",
        "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=20, p=0.6),\n",
        "    A.Perspective(scale=(0.05, 0.1), p=0.4),\n",
        "\n",
        "    # Simula diversi dispositivi di cattura\n",
        "    A.CLAHE(clip_limit=2.0, p=0.3),\n",
        "    A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.3),\n",
        "\n",
        "    # Augmentazioni base\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n",
        "\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "], additional_targets={'mask': 'mask'})\n",
        "\n",
        "\n",
        "test_tf = A.Compose([\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "], additional_targets={'mask': 'mask'})\n",
        "\n",
        "train_dataset = AntiSpoofDataset(train_color,train_depth,train_labels,train_tf)\n",
        "test_dataset = AntiSpoofDataset(test_color,test_depth,test_labels,test_tf)\n",
        "\n",
        "class_counts = [train_labels.count(0), train_labels.count(1)]\n",
        "class_weights = [1.0 / c for c in class_counts]\n",
        "sample_weights = [class_weights[label] for label in train_labels]\n",
        "sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=2,pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2,pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Architettura del Modello Anti-Spoofing\n",
        "\n",
        "Questa sezione definisce l'architettura completa del nostro modello di classificazione. Il design si basa su un approccio **multimodale** che sfrutta sia le informazioni cromatiche (RGB) sia quelle di profonditÃ  per massimizzare l'accuratezza. L'architettura Ã¨ composta da diversi moduli chiave:\n",
        "\n",
        "#### `RGBEncoder`\n",
        "Questo modulo Ã¨ responsabile dell'elaborazione delle immagini a colori. La sua architettura Ã¨ cosÃ¬ strutturata:\n",
        "-   **Backbone**: Utilizza le feature convoluzionali di **EfficientNet-B0**, pre-addestrato su ImageNet. Questa scelta fornisce un potente estrattore di feature senza un costo computazionale eccessivo.\n",
        "-   **`ChannelAttention`**: Dopo l'estrazione delle feature, viene applicato un meccanismo di attenzione sui canali. Questo modulo, ispirato a CBAM (Convolutional Block Attention Module), impara a pesare dinamicamente i canali delle feature map, permettendo al modello di concentrarsi sulle informazioni piÃ¹ rilevanti e sopprimere quelle meno utili.\n",
        "-   **Head di Proiezione**: Le feature raffinate vengono poi processate da un blocco di classificazione (head) composto da strati lineari, normalizzazione e dropout, che le proietta in uno spazio latente a 128 dimensioni.\n",
        "\n",
        "#### `DepthEncoder`\n",
        "Questo modulo, progettato su misura, elabora le mappe di profonditÃ  in scala di grigi. La sua architettura Ã¨ pensata per catturare informazioni a diverse scale:\n",
        "-   **Feature Extractor Multi-Scala**: Consiste in una serie di blocchi convoluzionali che estraggono feature a risoluzioni decrescenti (`f1`, `f2`, `f3`).\n",
        "-   **Fusione Multi-Scala**: Le feature map estratte a diverse profonditÃ  vengono ridimensionate alla stessa risoluzione spaziale e concatenate. Questo permette al modello di combinare dettagli fini (da feature map a risoluzione piÃ¹ alta) con informazioni contestuali piÃ¹ ampie (da feature map a risoluzione piÃ¹ bassa).\n",
        "-   **Head di Proiezione**: Similmente all'encoder RGB, un blocco finale proietta le feature di profonditÃ  fuse in uno spazio latente a 128 dimensioni.\n",
        "\n",
        "#### `CrossModalAttention`\n",
        "Questo Ã¨ il cuore dell'interazione tra le due modalitÃ . Vengono utilizzati due moduli di attenzione incrociata:\n",
        "1.  **`rgb_to_depth`**: Usa le feature RGB come *query* e le feature di profonditÃ  come *key* e *value*. In pratica, \"chiede\" alle feature di profonditÃ  quali siano le piÃ¹ rilevanti per contestualizzare le informazioni RGB.\n",
        "2.  **`depth_to_rgb`**: Fa l'opposto, usando la profonditÃ  come *query* per pesare le feature RGB.\n",
        "\n",
        "Questo meccanismo permette a ciascuna modalitÃ  di arricchirsi con le informazioni complementari fornite dall'altra.\n",
        "\n",
        "#### `AntiSpoofingModel` (Modello Principale)\n",
        "Il modello finale assembla i componenti precedenti:\n",
        "1.  **Estrazione Indipendente**: Le immagini RGB e di profonditÃ  vengono processate dai rispettivi encoder per ottenere i vettori di feature iniziali (`rgb_feat`, `depth_feat`).\n",
        "2.  **Arricchimento Incrociato**: Vengono calcolate le feature \"attese\" (`rgb_attended`, `depth_attended`) tramite i moduli di `CrossModalAttention`.\n",
        "3.  **Concatenazione e Fusione**: I quattro vettori di feature (originali e attesi) vengono concatenati in un unico vettore di dimensione 512.\n",
        "4.  **Classificazione Finale**: Un blocco di fusione, composto da strati densi con dropout e normalizzazione, processa il vettore combinato per produrre le feature finali a 128 dimensioni. Infine, un classificatore lineare produce i *logit* per le due classi (\"real\" e \"fake\").\n",
        "\n",
        "Il modello puÃ² restituire opzionalmente anche le feature finali, utili per funzioni di loss piÃ¹ complesse come la loss contrastiva."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "SI2a6Oa-cyU1"
      },
      "outputs": [],
      "source": [
        "class ChannelAttention(nn.Module):\n",
        "  def __init__(self,channels,reduction=16):\n",
        "    super().__init__()\n",
        "\n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d(1) # pool medio globale\n",
        "    self.max_pool = nn.AdaptiveMaxPool2d(1) # pool max globale\n",
        "\n",
        "    #Bottleneck\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(channels,channels//reduction,bias=False),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(channels//reduction,channels,bias=False)\n",
        "    )\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self,x):\n",
        "    b, c, _, _ = x.size()\n",
        "    avg_out = self.avg_pool(x).view(b, c)  # [B, 1280, 7, 7] â†’ [B, 1280, 1, 1] â†’ [B, 1280]\n",
        "    max_out = self.max_pool(x).view(b, c)  # [B, 1280, 7, 7] â†’ [B, 1280, 1, 1] â†’ [B, 1280]\n",
        "    avg_out = self.fc(avg_out)  # [B, 1280] â†’ [B, 80] â†’ [B, 1280]\n",
        "    max_out = self.fc(max_out)  # [B, 1280] â†’ [B, 80] â†’ [B, 1280]\n",
        "    out = self.sigmoid(avg_out + max_out)  # [B, 1280] â†’ somma â†’ sigmoid\n",
        "    out = out.view(b, c, 1, 1)              # [B, 1280, 1, 1]\n",
        "    return x * out  # [B, 1280, 7, 7] * [B, 1280, 1, 1] â†’ [B, 1280, 7, 7]\n",
        "\n",
        "\n",
        "class RGBEncoder(nn.Module):\n",
        "  def __init__(self,output_dim=128):\n",
        "    super().__init__()\n",
        "\n",
        "    base = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "\n",
        "    #backbone dell'encoder RGB --> EfficentNet-B0 net addestrato su ImageNet ([B,3,224,224] --> [B,1280,7,7])\n",
        "    self.feature_extractor = base.features\n",
        "    #([B,1280,7,7] --> [B,1280,7,7])\n",
        "    self.attention = ChannelAttention(1280)\n",
        "    # ([B,1280],[B120])\n",
        "    self.head = nn.Sequential(\n",
        "        nn.Linear(1280, 256),\n",
        "        nn.BatchNorm1d(256),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(256, 128)\n",
        "    )\n",
        "\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.feature_extractor(x)\n",
        "    x = self.attention(x)\n",
        "    x = F.adaptive_avg_pool2d(x,1).flatten(1) #([B,1280,7,7] --> [B,1280])\n",
        "    return self.head(x)\n",
        "\n",
        "class DepthEncoder(nn.Module):\n",
        "  def __init__(self,output_dim=128):\n",
        "    super().__init__()\n",
        "\n",
        "    # [B,1,224,224] â†’ [B,32,112,112]\n",
        "    self.depth1 = nn.Sequential(\n",
        "        nn.Conv2d(1,32,3,2,1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32,32,3,1,1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    # [B,32,112,112] --> [B,64,56,56]\n",
        "    self.depth2 = nn.Sequential(\n",
        "        nn.Conv2d(32,64,3,2,1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64,64,3,1,1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    # [B,64,56,56] --> [B,128,28,28]\n",
        "    self.depth3 = nn.Sequential(\n",
        "        nn.Conv2d(64,128,3,2,1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.fusion = nn.Sequential(\n",
        "        nn.Conv2d(224,128,1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.upsample1 = nn.Upsample(scale_factor=4,mode='bilinear',align_corners=False)\n",
        "    self.upsample2 = nn.Upsample(scale_factor=2,mode='bilinear',align_corners=False)\n",
        "\n",
        "    self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(128,128)\n",
        "    )\n",
        "\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "  def forward(self,x):\n",
        "    f1 = self.depth1(x)   # [B,1,224,224] â†’ [B,32,112,112]\n",
        "    f2 = self.depth2(f1)  # âœ… Usa f1 (32 canali)\n",
        "    f3 = self.depth3(f2)  # âœ… Usa f2 (64 canali)\n",
        "\n",
        "    f1_up = self.upsample1(f1) # [B,32,112,112] --> [B,32,448,448]\n",
        "    f2_up = self.upsample2(f2) # [B,64,56,56] --> [B,64,112,112]\n",
        "\n",
        "    target_size = f3.shape[2:] # (28,28)\n",
        "    f1_up = F.interpolate(f1_up,size=target_size,mode='bilinear',align_corners=False) # [B,32,448,448] --> [b,32,64,64]\n",
        "    f2_up = F.interpolate(f2_up,size=target_size,mode='bilinear',align_corners=False) # [B,64,112,112] --> [B,64,28,28]\n",
        "\n",
        "    #concatenazione multi-scala\n",
        "    multi_scale = torch.cat([f1_up,f2_up,f3],dim=1) # [B,32,28,28] + [B,64,28,28] + [B,128,28,28] = [B,224,28,28]\n",
        "\n",
        "    fused = self.fusion(multi_scale) # [B,224,28,28] --> [B,128,28,28]\n",
        "\n",
        "    x = F.adaptive_avg_pool2d(fused, 1).flatten(1)  # [B,128,28,28] â†’ [B,128]\n",
        "    return self.fc(x)  # [B,128]\n",
        "\n",
        "\n",
        "class CrossModalAttention(nn.Module):\n",
        "  def __init__(self,dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.query = nn.Linear(dim,dim)\n",
        "    self.key = nn.Linear(dim,dim)\n",
        "    self.value = nn.Linear(dim,dim)\n",
        "    self.scale = dim ** -0.5\n",
        "    self.out_proj = nn.Linear(dim, dim)\n",
        "\n",
        "  def forward(self,rgb_x,depth_x):\n",
        "    q = self.query(rgb_x).unsqueeze(1) # [B, 128] â†’ [B, 1, 128]\n",
        "    k = self.key(depth_x).unsqueeze(1) # [B, 128] â†’ [B, 1, 128]\n",
        "    v = self.value(depth_x).unsqueeze(1) # [B, 128] â†’ [B, 1, 128]\n",
        "\n",
        "    attn = (q @ k.transpose(-2,-1)) * self.scale\n",
        "    attn = F.softmax(attn,dim=-1) # normalizzo in [0,1]\n",
        "    out = (attn @ v).squeeze(1)  # [B, 1, 1] @ [B, 1, 128] â†’ [B, 128]\n",
        "    return self.out_proj(out)\n",
        "\n",
        "\n",
        "class AntiSpoofingModel(nn.Module):\n",
        "  def __init__(self,rgb_encoder,depth_encoder):\n",
        "    super().__init__()\n",
        "    self.rgb_encoder = rgb_encoder\n",
        "    self.depth_encoder = depth_encoder\n",
        "\n",
        "    self.rgb_to_depth = CrossModalAttention(rgb_encoder.output_dim)\n",
        "    self.depth_to_rgb = CrossModalAttention(depth_encoder.output_dim)\n",
        "\n",
        "    fusion_dim = rgb_encoder.output_dim * 2 + depth_encoder.output_dim * 2\n",
        "    self.fusion = nn.Sequential(\n",
        "        nn.Linear(fusion_dim, 512),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.BatchNorm1d(256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.4),\n",
        "        nn.Linear(256, 128),\n",
        "        nn.BatchNorm1d(128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "    )\n",
        "\n",
        "    self.classifier = nn.Linear(128,2)\n",
        "\n",
        "  def forward(self,rgb_x,depth_x,return_features=False):\n",
        "    rgb_feat = self.rgb_encoder(rgb_x)\n",
        "    depth_feat = self.depth_encoder(depth_x)\n",
        "\n",
        "    rgb_attended = self.rgb_to_depth(rgb_feat,depth_feat)\n",
        "    depth_attended = self.depth_to_rgb(depth_feat,rgb_feat)\n",
        "\n",
        "    x = torch.cat(\n",
        "        [\n",
        "            rgb_feat,rgb_attended,\n",
        "            depth_feat,depth_attended\n",
        "        ],\n",
        "        dim=1\n",
        "    )\n",
        "\n",
        "    features = self.fusion(x)\n",
        "    logits = self.classifier(features)\n",
        "\n",
        "    if return_features:\n",
        "            return logits, features\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWOxCQN3Ga0G",
        "outputId": "47feda8a-387e-4420-d85f-5a9f7c7d88b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================================================================================================\n",
            "Layer (type:depth-idx)                                       Input Shape               Output Shape              Param #                   Trainable\n",
            "================================================================================================================================================================\n",
            "AntiSpoofingModel                                            [1, 3, 224, 224]          [1, 2]                    --                        True\n",
            "â”œâ”€RGBEncoder: 1-1                                            [1, 3, 224, 224]          [1, 128]                  --                        True\n",
            "â”‚    â””â”€Sequential: 2-1                                       [1, 3, 224, 224]          [1, 1280, 7, 7]           --                        True\n",
            "â”‚    â”‚    â””â”€Conv2dNormActivation: 3-1                        [1, 3, 224, 224]          [1, 32, 112, 112]         928                       True\n",
            "â”‚    â”‚    â””â”€Sequential: 3-2                                  [1, 32, 112, 112]         [1, 16, 112, 112]         1,448                     True\n",
            "â”‚    â”‚    â””â”€Sequential: 3-3                                  [1, 16, 112, 112]         [1, 24, 56, 56]           16,714                    True\n",
            "â”‚    â”‚    â””â”€Sequential: 3-4                                  [1, 24, 56, 56]           [1, 40, 28, 28]           46,640                    True\n",
            "â”‚    â”‚    â””â”€Sequential: 3-5                                  [1, 40, 28, 28]           [1, 80, 14, 14]           242,930                   True\n",
            "â”‚    â”‚    â””â”€Sequential: 3-6                                  [1, 80, 14, 14]           [1, 112, 14, 14]          543,148                   True\n",
            "â”‚    â”‚    â””â”€Sequential: 3-7                                  [1, 112, 14, 14]          [1, 192, 7, 7]            2,026,348                 True\n",
            "â”‚    â”‚    â””â”€Sequential: 3-8                                  [1, 192, 7, 7]            [1, 320, 7, 7]            717,232                   True\n",
            "â”‚    â”‚    â””â”€Conv2dNormActivation: 3-9                        [1, 320, 7, 7]            [1, 1280, 7, 7]           412,160                   True\n",
            "â”‚    â””â”€ChannelAttention: 2-2                                 [1, 1280, 7, 7]           [1, 1280, 7, 7]           --                        True\n",
            "â”‚    â”‚    â””â”€AdaptiveAvgPool2d: 3-10                          [1, 1280, 7, 7]           [1, 1280, 1, 1]           --                        --\n",
            "â”‚    â”‚    â””â”€AdaptiveMaxPool2d: 3-11                          [1, 1280, 7, 7]           [1, 1280, 1, 1]           --                        --\n",
            "â”‚    â”‚    â””â”€Sequential: 3-12                                 [1, 1280]                 [1, 1280]                 204,800                   True\n",
            "â”‚    â”‚    â””â”€Sequential: 3-13                                 [1, 1280]                 [1, 1280]                 (recursive)               True\n",
            "â”‚    â”‚    â””â”€Sigmoid: 3-14                                    [1, 1280]                 [1, 1280]                 --                        --\n",
            "â”‚    â””â”€Sequential: 2-3                                       [1, 1280]                 [1, 128]                  --                        True\n",
            "â”‚    â”‚    â””â”€Linear: 3-15                                     [1, 1280]                 [1, 256]                  327,936                   True\n",
            "â”‚    â”‚    â””â”€BatchNorm1d: 3-16                                [1, 256]                  [1, 256]                  512                       True\n",
            "â”‚    â”‚    â””â”€ReLU: 3-17                                       [1, 256]                  [1, 256]                  --                        --\n",
            "â”‚    â”‚    â””â”€Dropout: 3-18                                    [1, 256]                  [1, 256]                  --                        --\n",
            "â”‚    â”‚    â””â”€Linear: 3-19                                     [1, 256]                  [1, 128]                  32,896                    True\n",
            "â”œâ”€DepthEncoder: 1-2                                          [1, 1, 224, 224]          [1, 128]                  --                        True\n",
            "â”‚    â””â”€Sequential: 2-4                                       [1, 1, 224, 224]          [1, 32, 112, 112]         --                        True\n",
            "â”‚    â”‚    â””â”€Conv2d: 3-20                                     [1, 1, 224, 224]          [1, 32, 112, 112]         320                       True\n",
            "â”‚    â”‚    â””â”€BatchNorm2d: 3-21                                [1, 32, 112, 112]         [1, 32, 112, 112]         64                        True\n",
            "â”‚    â”‚    â””â”€ReLU: 3-22                                       [1, 32, 112, 112]         [1, 32, 112, 112]         --                        --\n",
            "â”‚    â”‚    â””â”€Conv2d: 3-23                                     [1, 32, 112, 112]         [1, 32, 112, 112]         9,248                     True\n",
            "â”‚    â”‚    â””â”€BatchNorm2d: 3-24                                [1, 32, 112, 112]         [1, 32, 112, 112]         64                        True\n",
            "â”‚    â”‚    â””â”€ReLU: 3-25                                       [1, 32, 112, 112]         [1, 32, 112, 112]         --                        --\n",
            "â”‚    â””â”€Sequential: 2-5                                       [1, 32, 112, 112]         [1, 64, 56, 56]           --                        True\n",
            "â”‚    â”‚    â””â”€Conv2d: 3-26                                     [1, 32, 112, 112]         [1, 64, 56, 56]           18,496                    True\n",
            "â”‚    â”‚    â””â”€BatchNorm2d: 3-27                                [1, 64, 56, 56]           [1, 64, 56, 56]           128                       True\n",
            "â”‚    â”‚    â””â”€ReLU: 3-28                                       [1, 64, 56, 56]           [1, 64, 56, 56]           --                        --\n",
            "â”‚    â”‚    â””â”€Conv2d: 3-29                                     [1, 64, 56, 56]           [1, 64, 56, 56]           36,928                    True\n",
            "â”‚    â”‚    â””â”€BatchNorm2d: 3-30                                [1, 64, 56, 56]           [1, 64, 56, 56]           128                       True\n",
            "â”‚    â”‚    â””â”€ReLU: 3-31                                       [1, 64, 56, 56]           [1, 64, 56, 56]           --                        --\n",
            "â”‚    â””â”€Sequential: 2-6                                       [1, 64, 56, 56]           [1, 128, 28, 28]          --                        True\n",
            "â”‚    â”‚    â””â”€Conv2d: 3-32                                     [1, 64, 56, 56]           [1, 128, 28, 28]          73,856                    True\n",
            "â”‚    â”‚    â””â”€BatchNorm2d: 3-33                                [1, 128, 28, 28]          [1, 128, 28, 28]          256                       True\n",
            "â”‚    â”‚    â””â”€ReLU: 3-34                                       [1, 128, 28, 28]          [1, 128, 28, 28]          --                        --\n",
            "â”‚    â””â”€Upsample: 2-7                                         [1, 32, 112, 112]         [1, 32, 448, 448]         --                        --\n",
            "â”‚    â””â”€Upsample: 2-8                                         [1, 64, 56, 56]           [1, 64, 112, 112]         --                        --\n",
            "â”‚    â””â”€Sequential: 2-9                                       [1, 224, 28, 28]          [1, 128, 28, 28]          --                        True\n",
            "â”‚    â”‚    â””â”€Conv2d: 3-35                                     [1, 224, 28, 28]          [1, 128, 28, 28]          28,800                    True\n",
            "â”‚    â”‚    â””â”€BatchNorm2d: 3-36                                [1, 128, 28, 28]          [1, 128, 28, 28]          256                       True\n",
            "â”‚    â”‚    â””â”€ReLU: 3-37                                       [1, 128, 28, 28]          [1, 128, 28, 28]          --                        --\n",
            "â”‚    â””â”€Sequential: 2-10                                      [1, 128]                  [1, 128]                  --                        True\n",
            "â”‚    â”‚    â””â”€Linear: 3-38                                     [1, 128]                  [1, 128]                  16,512                    True\n",
            "â”œâ”€CrossModalAttention: 1-3                                   [1, 128]                  [1, 128]                  --                        True\n",
            "â”‚    â””â”€Linear: 2-11                                          [1, 128]                  [1, 128]                  16,512                    True\n",
            "â”‚    â””â”€Linear: 2-12                                          [1, 128]                  [1, 128]                  16,512                    True\n",
            "â”‚    â””â”€Linear: 2-13                                          [1, 128]                  [1, 128]                  16,512                    True\n",
            "â”‚    â””â”€Linear: 2-14                                          [1, 128]                  [1, 128]                  16,512                    True\n",
            "â”œâ”€CrossModalAttention: 1-4                                   [1, 128]                  [1, 128]                  --                        True\n",
            "â”‚    â””â”€Linear: 2-15                                          [1, 128]                  [1, 128]                  16,512                    True\n",
            "â”‚    â””â”€Linear: 2-16                                          [1, 128]                  [1, 128]                  16,512                    True\n",
            "â”‚    â””â”€Linear: 2-17                                          [1, 128]                  [1, 128]                  16,512                    True\n",
            "â”‚    â””â”€Linear: 2-18                                          [1, 128]                  [1, 128]                  16,512                    True\n",
            "â”œâ”€Sequential: 1-5                                            [1, 512]                  [1, 128]                  --                        True\n",
            "â”‚    â””â”€Linear: 2-19                                          [1, 512]                  [1, 512]                  262,656                   True\n",
            "â”‚    â””â”€BatchNorm1d: 2-20                                     [1, 512]                  [1, 512]                  1,024                     True\n",
            "â”‚    â””â”€ReLU: 2-21                                            [1, 512]                  [1, 512]                  --                        --\n",
            "â”‚    â””â”€Dropout: 2-22                                         [1, 512]                  [1, 512]                  --                        --\n",
            "â”‚    â””â”€Linear: 2-23                                          [1, 512]                  [1, 256]                  131,328                   True\n",
            "â”‚    â””â”€BatchNorm1d: 2-24                                     [1, 256]                  [1, 256]                  512                       True\n",
            "â”‚    â””â”€ReLU: 2-25                                            [1, 256]                  [1, 256]                  --                        --\n",
            "â”‚    â””â”€Dropout: 2-26                                         [1, 256]                  [1, 256]                  --                        --\n",
            "â”‚    â””â”€Linear: 2-27                                          [1, 256]                  [1, 128]                  32,896                    True\n",
            "â”‚    â””â”€BatchNorm1d: 2-28                                     [1, 128]                  [1, 128]                  256                       True\n",
            "â”‚    â””â”€ReLU: 2-29                                            [1, 128]                  [1, 128]                  --                        --\n",
            "â”‚    â””â”€Dropout: 2-30                                         [1, 128]                  [1, 128]                  --                        --\n",
            "â”œâ”€Linear: 1-6                                                [1, 128]                  [1, 2]                    258                       True\n",
            "================================================================================================================================================================\n",
            "Total params: 5,319,774\n",
            "Trainable params: 5,319,774\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.MEGABYTES): 760.25\n",
            "================================================================================================================================================================\n",
            "Input size (MB): 0.80\n",
            "Forward/backward pass size (MB): 130.41\n",
            "Params size (MB): 21.28\n",
            "Estimated Total Size (MB): 152.49\n",
            "================================================================================================================================================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "================================================================================================================================================================\n",
              "Layer (type:depth-idx)                                       Input Shape               Output Shape              Param #                   Trainable\n",
              "================================================================================================================================================================\n",
              "AntiSpoofingModel                                            [1, 3, 224, 224]          [1, 2]                    --                        True\n",
              "â”œâ”€RGBEncoder: 1-1                                            [1, 3, 224, 224]          [1, 128]                  --                        True\n",
              "â”‚    â””â”€Sequential: 2-1                                       [1, 3, 224, 224]          [1, 1280, 7, 7]           --                        True\n",
              "â”‚    â”‚    â””â”€Conv2dNormActivation: 3-1                        [1, 3, 224, 224]          [1, 32, 112, 112]         928                       True\n",
              "â”‚    â”‚    â””â”€Sequential: 3-2                                  [1, 32, 112, 112]         [1, 16, 112, 112]         1,448                     True\n",
              "â”‚    â”‚    â””â”€Sequential: 3-3                                  [1, 16, 112, 112]         [1, 24, 56, 56]           16,714                    True\n",
              "â”‚    â”‚    â””â”€Sequential: 3-4                                  [1, 24, 56, 56]           [1, 40, 28, 28]           46,640                    True\n",
              "â”‚    â”‚    â””â”€Sequential: 3-5                                  [1, 40, 28, 28]           [1, 80, 14, 14]           242,930                   True\n",
              "â”‚    â”‚    â””â”€Sequential: 3-6                                  [1, 80, 14, 14]           [1, 112, 14, 14]          543,148                   True\n",
              "â”‚    â”‚    â””â”€Sequential: 3-7                                  [1, 112, 14, 14]          [1, 192, 7, 7]            2,026,348                 True\n",
              "â”‚    â”‚    â””â”€Sequential: 3-8                                  [1, 192, 7, 7]            [1, 320, 7, 7]            717,232                   True\n",
              "â”‚    â”‚    â””â”€Conv2dNormActivation: 3-9                        [1, 320, 7, 7]            [1, 1280, 7, 7]           412,160                   True\n",
              "â”‚    â””â”€ChannelAttention: 2-2                                 [1, 1280, 7, 7]           [1, 1280, 7, 7]           --                        True\n",
              "â”‚    â”‚    â””â”€AdaptiveAvgPool2d: 3-10                          [1, 1280, 7, 7]           [1, 1280, 1, 1]           --                        --\n",
              "â”‚    â”‚    â””â”€AdaptiveMaxPool2d: 3-11                          [1, 1280, 7, 7]           [1, 1280, 1, 1]           --                        --\n",
              "â”‚    â”‚    â””â”€Sequential: 3-12                                 [1, 1280]                 [1, 1280]                 204,800                   True\n",
              "â”‚    â”‚    â””â”€Sequential: 3-13                                 [1, 1280]                 [1, 1280]                 (recursive)               True\n",
              "â”‚    â”‚    â””â”€Sigmoid: 3-14                                    [1, 1280]                 [1, 1280]                 --                        --\n",
              "â”‚    â””â”€Sequential: 2-3                                       [1, 1280]                 [1, 128]                  --                        True\n",
              "â”‚    â”‚    â””â”€Linear: 3-15                                     [1, 1280]                 [1, 256]                  327,936                   True\n",
              "â”‚    â”‚    â””â”€BatchNorm1d: 3-16                                [1, 256]                  [1, 256]                  512                       True\n",
              "â”‚    â”‚    â””â”€ReLU: 3-17                                       [1, 256]                  [1, 256]                  --                        --\n",
              "â”‚    â”‚    â””â”€Dropout: 3-18                                    [1, 256]                  [1, 256]                  --                        --\n",
              "â”‚    â”‚    â””â”€Linear: 3-19                                     [1, 256]                  [1, 128]                  32,896                    True\n",
              "â”œâ”€DepthEncoder: 1-2                                          [1, 1, 224, 224]          [1, 128]                  --                        True\n",
              "â”‚    â””â”€Sequential: 2-4                                       [1, 1, 224, 224]          [1, 32, 112, 112]         --                        True\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-20                                     [1, 1, 224, 224]          [1, 32, 112, 112]         320                       True\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-21                                [1, 32, 112, 112]         [1, 32, 112, 112]         64                        True\n",
              "â”‚    â”‚    â””â”€ReLU: 3-22                                       [1, 32, 112, 112]         [1, 32, 112, 112]         --                        --\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-23                                     [1, 32, 112, 112]         [1, 32, 112, 112]         9,248                     True\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-24                                [1, 32, 112, 112]         [1, 32, 112, 112]         64                        True\n",
              "â”‚    â”‚    â””â”€ReLU: 3-25                                       [1, 32, 112, 112]         [1, 32, 112, 112]         --                        --\n",
              "â”‚    â””â”€Sequential: 2-5                                       [1, 32, 112, 112]         [1, 64, 56, 56]           --                        True\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-26                                     [1, 32, 112, 112]         [1, 64, 56, 56]           18,496                    True\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-27                                [1, 64, 56, 56]           [1, 64, 56, 56]           128                       True\n",
              "â”‚    â”‚    â””â”€ReLU: 3-28                                       [1, 64, 56, 56]           [1, 64, 56, 56]           --                        --\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-29                                     [1, 64, 56, 56]           [1, 64, 56, 56]           36,928                    True\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-30                                [1, 64, 56, 56]           [1, 64, 56, 56]           128                       True\n",
              "â”‚    â”‚    â””â”€ReLU: 3-31                                       [1, 64, 56, 56]           [1, 64, 56, 56]           --                        --\n",
              "â”‚    â””â”€Sequential: 2-6                                       [1, 64, 56, 56]           [1, 128, 28, 28]          --                        True\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-32                                     [1, 64, 56, 56]           [1, 128, 28, 28]          73,856                    True\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-33                                [1, 128, 28, 28]          [1, 128, 28, 28]          256                       True\n",
              "â”‚    â”‚    â””â”€ReLU: 3-34                                       [1, 128, 28, 28]          [1, 128, 28, 28]          --                        --\n",
              "â”‚    â””â”€Upsample: 2-7                                         [1, 32, 112, 112]         [1, 32, 448, 448]         --                        --\n",
              "â”‚    â””â”€Upsample: 2-8                                         [1, 64, 56, 56]           [1, 64, 112, 112]         --                        --\n",
              "â”‚    â””â”€Sequential: 2-9                                       [1, 224, 28, 28]          [1, 128, 28, 28]          --                        True\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-35                                     [1, 224, 28, 28]          [1, 128, 28, 28]          28,800                    True\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-36                                [1, 128, 28, 28]          [1, 128, 28, 28]          256                       True\n",
              "â”‚    â”‚    â””â”€ReLU: 3-37                                       [1, 128, 28, 28]          [1, 128, 28, 28]          --                        --\n",
              "â”‚    â””â”€Sequential: 2-10                                      [1, 128]                  [1, 128]                  --                        True\n",
              "â”‚    â”‚    â””â”€Linear: 3-38                                     [1, 128]                  [1, 128]                  16,512                    True\n",
              "â”œâ”€CrossModalAttention: 1-3                                   [1, 128]                  [1, 128]                  --                        True\n",
              "â”‚    â””â”€Linear: 2-11                                          [1, 128]                  [1, 128]                  16,512                    True\n",
              "â”‚    â””â”€Linear: 2-12                                          [1, 128]                  [1, 128]                  16,512                    True\n",
              "â”‚    â””â”€Linear: 2-13                                          [1, 128]                  [1, 128]                  16,512                    True\n",
              "â”‚    â””â”€Linear: 2-14                                          [1, 128]                  [1, 128]                  16,512                    True\n",
              "â”œâ”€CrossModalAttention: 1-4                                   [1, 128]                  [1, 128]                  --                        True\n",
              "â”‚    â””â”€Linear: 2-15                                          [1, 128]                  [1, 128]                  16,512                    True\n",
              "â”‚    â””â”€Linear: 2-16                                          [1, 128]                  [1, 128]                  16,512                    True\n",
              "â”‚    â””â”€Linear: 2-17                                          [1, 128]                  [1, 128]                  16,512                    True\n",
              "â”‚    â””â”€Linear: 2-18                                          [1, 128]                  [1, 128]                  16,512                    True\n",
              "â”œâ”€Sequential: 1-5                                            [1, 512]                  [1, 128]                  --                        True\n",
              "â”‚    â””â”€Linear: 2-19                                          [1, 512]                  [1, 512]                  262,656                   True\n",
              "â”‚    â””â”€BatchNorm1d: 2-20                                     [1, 512]                  [1, 512]                  1,024                     True\n",
              "â”‚    â””â”€ReLU: 2-21                                            [1, 512]                  [1, 512]                  --                        --\n",
              "â”‚    â””â”€Dropout: 2-22                                         [1, 512]                  [1, 512]                  --                        --\n",
              "â”‚    â””â”€Linear: 2-23                                          [1, 512]                  [1, 256]                  131,328                   True\n",
              "â”‚    â””â”€BatchNorm1d: 2-24                                     [1, 256]                  [1, 256]                  512                       True\n",
              "â”‚    â””â”€ReLU: 2-25                                            [1, 256]                  [1, 256]                  --                        --\n",
              "â”‚    â””â”€Dropout: 2-26                                         [1, 256]                  [1, 256]                  --                        --\n",
              "â”‚    â””â”€Linear: 2-27                                          [1, 256]                  [1, 128]                  32,896                    True\n",
              "â”‚    â””â”€BatchNorm1d: 2-28                                     [1, 128]                  [1, 128]                  256                       True\n",
              "â”‚    â””â”€ReLU: 2-29                                            [1, 128]                  [1, 128]                  --                        --\n",
              "â”‚    â””â”€Dropout: 2-30                                         [1, 128]                  [1, 128]                  --                        --\n",
              "â”œâ”€Linear: 1-6                                                [1, 128]                  [1, 2]                    258                       True\n",
              "================================================================================================================================================================\n",
              "Total params: 5,319,774\n",
              "Trainable params: 5,319,774\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 760.25\n",
              "================================================================================================================================================================\n",
              "Input size (MB): 0.80\n",
              "Forward/backward pass size (MB): 130.41\n",
              "Params size (MB): 21.28\n",
              "Estimated Total Size (MB): 152.49\n",
              "================================================================================================================================================================"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = AntiSpoofingModel(\n",
        "    RGBEncoder(),\n",
        "    DepthEncoder()\n",
        ").to(device)\n",
        "\n",
        "# âœ… CORRETTO: specifica input_data con tuple di tensors\n",
        "summary(\n",
        "    model,\n",
        "    input_data=[\n",
        "        torch.randn(1, 3, 224, 224).to(device),   # RGB input\n",
        "        torch.randn(1, 1, 224, 224).to(device)    # Depth input\n",
        "    ],\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    depth=3,\n",
        "    verbose=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Funzione di Loss Ibrida: `ContrastativeFocalLoss`\n",
        "\n",
        "Per addestrare efficacemente il nostro modello, non ci affidiamo a una semplice Cross-Entropy, ma implementiamo una **funzione di loss ibrida** personalizzata che combina due potenti concetti: la **Focal Loss** e la **Contrastive Loss**. Questa scelta Ã¨ motivata dalla necessitÃ  di affrontare due sfide comuni: lo sbilanciamento delle classi e la difficoltÃ  nel distinguere esempi \"difficili\".\n",
        "\n",
        "#### Focal Loss\n",
        "La componente `focal_loss` affronta il problema dello sbilanciamento delle classi e la presenza di esempi \"facili\" e \"difficili\". A differenza della Cross-Entropy standard, la Focal Loss riduce dinamicamente il peso degli esempi che il modello classifica giÃ  correttamente (quelli \"facili\"). Questo permette all'ottimizzatore di **concentrarsi sugli esempi piÃ¹ difficili e informativi**, che sono spesso quelli al confine tra le classi \"real\" e \"fake\". I parametri chiave sono:\n",
        "-   `alpha`: Pesa l'importanza delle classi positive/negative.\n",
        "-   `gamma`: Modula la focalizzazione; un `gamma` piÃ¹ alto aumenta l'effetto di down-weighting sugli esempi facili.\n",
        "\n",
        "#### Contrastive Loss (Supervised)\n",
        "La componente `contrastive_loss` opera non sui *logit* finali, ma direttamente sullo **spazio delle feature** a 128 dimensioni prodotte dal modello. L'obiettivo Ã¨ quello di strutturare questo spazio latente in modo che sia semanticamente significativo. In particolare, questa loss:\n",
        "-   **Avvicina** le feature di campioni appartenenti alla stessa classe (es. due volti \"real\" diversi).\n",
        "-   **Allontana** le feature di campioni appartenenti a classi diverse (es. un volto \"real\" e uno \"fake\").\n",
        "\n",
        "Questo approccio, noto come *Supervised Contrastive Learning*, aiuta il modello a imparare rappresentazioni piÃ¹ robuste e discriminative, migliorando la sua capacitÃ  di generalizzazione. Il parametro `temperature` controlla la separazione tra le classi nello spazio latente.\n",
        "\n",
        "#### Combinazione\n",
        "La loss finale Ã¨ una somma pesata delle due componenti:\n",
        "`Total Loss = focal_loss + contrast_weight * contrastive_loss`\n",
        "\n",
        "Durante l'**addestramento**, vengono utilizzate entrambe le componenti per ottimizzare sia la classificazione sia la struttura dello spazio delle feature. Durante la **valutazione**, viene usata solo la `focal_loss` per calcolare una metrica di performance coerente, poichÃ© la componente contrastiva dipende dalla composizione del batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m8dYaSrelvD"
      },
      "outputs": [],
      "source": [
        "class ContrastativeFocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, contrast_weight=0.3, temperature=0.5):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.contrast_weight = contrast_weight\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def focal_loss(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "    def contrastive_loss(self, features, labels):\n",
        "        # Normalizza features per calcolare similaritÃ \n",
        "        features = F.normalize(features, dim=1)\n",
        "\n",
        "        # Matrice di similaritÃ  (cosine similarity)\n",
        "        sim_matrix = torch.matmul(features, features.T) / self.temperature\n",
        "\n",
        "        # Maschera per positive pairs (stessa classe)\n",
        "        labels = labels.view(-1, 1)\n",
        "        mask = torch.eq(labels, labels.T).float().to(features.device)\n",
        "\n",
        "        # Rimuovi diagonale (non confrontare sample con se stesso)\n",
        "        mask = mask - torch.eye(mask.size(0)).to(features.device)\n",
        "\n",
        "        # Contrastive loss: avvicina same-class, allontana different-class\n",
        "        exp_sim = torch.exp(sim_matrix)\n",
        "        log_prob = sim_matrix - torch.log(exp_sim.sum(1, keepdim=True) + 1e-8)\n",
        "\n",
        "        # Media solo sui positive pairs\n",
        "        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-8)\n",
        "        loss = -mean_log_prob_pos.mean()\n",
        "        return loss\n",
        "\n",
        "    def forward(self, logits, features=None, targets=None):\n",
        "        # Caso: criterion(out, y)\n",
        "        if targets is None and features is not None and features.dtype == torch.long:\n",
        "            targets = features\n",
        "            features = None\n",
        "\n",
        "        # Solo focal loss (test)\n",
        "        if features is None:\n",
        "            return self.focal_loss(logits, targets)\n",
        "\n",
        "        # Focal + contrastive (train)\n",
        "        focal = self.focal_loss(logits, targets)\n",
        "        contrastive = self.contrastive_loss(features, targets)\n",
        "        return focal + self.contrast_weight * contrastive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ottimizzatore, Scheduler e Early Stopping\n",
        "\n",
        "In questa sezione, configuriamo tutti gli strumenti necessari per il processo di addestramento: l'ottimizzatore che aggiorna i pesi del modello, lo scheduler che gestisce il learning rate e un meccanismo di early stopping per prevenire l'overfitting e salvare il modello migliore.\n",
        "\n",
        "#### Ottimizzatore e Funzione di Loss\n",
        "-   **Ottimizzatore `AdamW`**: Ãˆ stata scelta una variante dell'ottimizzatore Adam, nota come AdamW. A differenza di Adam classico, AdamW disaccoppia il meccanismo di weight decay dalla gradiente, portando spesso a una migliore generalizzazione e performance piÃ¹ stabili. L'opzione `amsgrad` Ã¨ attivata per migliorare ulteriormente la convergenza.\n",
        "-   **Istanza della Loss**: Viene creata un'istanza della nostra funzione di loss personalizzata, `ContrastativeFocalLoss`, con i parametri `alpha`, `gamma` e `contrast_weight` specificati.\n",
        "\n",
        "#### Scheduler del Learning Rate: `CosineAnnealingWarmRestarts`\n",
        "Per evitare di rimanere bloccati in minimi locali e per esplorare meglio lo spazio della loss, viene utilizzato uno scheduler avanzato. Il `CosineAnnealingWarmRestarts` varia ciclicamente il learning rate (LR) seguendo un andamento cosinusoidale:\n",
        "-   **Decadimento Coseno**: L'LR diminuisce gradualmente da un valore massimo a un valore minimo (`eta_min`) in un certo numero di epoche (`T_0`).\n",
        "-   **Warm Restarts**: Al termine di ogni ciclo, l'LR viene \"resettato\" al suo valore iniziale. Questo \"calcio\" aiuta il modello a uscire da eventuali plateau e a convergere verso soluzioni migliori.\n",
        "-   **Cicli Crescenti**: Il parametro `T_mult=2` fa sÃ¬ che la durata di ogni ciclo successivo raddoppi, permettendo al modello di affinare la sua convergenza man mano che l'addestramento procede.\n",
        "\n",
        "#### Classe `EarlyStopping`\n",
        "Per evitare di addestrare il modello piÃ¹ del necessario (rischiando overfitting e spreco di tempo), viene implementata una classe `EarlyStopping`. Questa utility monitora una metrica di performance sul set di validazione (nel nostro caso, l'F1-score) e interrompe l'addestramento se non si osservano miglioramenti per un numero specificato di epoche (`patience`).\n",
        "\n",
        "Le sue responsabilitÃ  principali sono:\n",
        "-   **Monitorare lo Score**: Confronta lo score dell'epoca corrente con il miglior score ottenuto finora.\n",
        "-   **Salvare il Modello Migliore**: Se viene raggiunto un nuovo score migliore, salva un checkpoint del modello, dello stato dell'ottimizzatore e delle metriche correnti.\n",
        "-   **Interrompere l'Addestramento**: Se non ci sono miglioramenti per `patience` epoche consecutive, imposta un flag per terminare il ciclo di training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsFpWx0gLQJh"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=15, min_delta=0, mode='max', save_path='best_model.pth'):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.mode = mode\n",
        "        self.save_path = save_path\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, score, model, optimizer, epoch, **kwargs):  #  Aggiunto **kwargs\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(model, optimizer, epoch, score, **kwargs)\n",
        "            return False\n",
        "\n",
        "        # Fix logica improved\n",
        "        if self.mode == \"max\":\n",
        "            improved = score > (self.best_score + self.min_delta)\n",
        "        else:  # mode == \"min\"\n",
        "            improved = score < (self.best_score - self.min_delta)  #  Corretto\n",
        "\n",
        "        if improved:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "            self.save_checkpoint(model, optimizer, epoch, score, **kwargs)\n",
        "            return False\n",
        "        else:\n",
        "            self.counter += 1  # Aggiunto self.\n",
        "            print(f\"   â³ Nessun miglioramento (counter {self.counter}/{self.patience})\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                return True\n",
        "            return False\n",
        "\n",
        "    def save_checkpoint(self, model, optimizer, epoch, score, **kwargs):\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'score': score,\n",
        "            **kwargs\n",
        "        }\n",
        "        torch.save(checkpoint, self.save_path)\n",
        "        print(f\"   ðŸ’¾ Checkpoint salvato! Score: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "jhvBXuhM_H6e"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.AdamW(\n",
        "  model.parameters(),\n",
        "  lr,\n",
        "  betas=(0.9, 0.999),\n",
        "  weight_decay=weight_decay,\n",
        "  amsgrad=True\n",
        ")\n",
        "\n",
        "scheduler = CosineAnnealingWarmRestarts(\n",
        "    optimizer,\n",
        "    T_0=10,  # Restart ogni 10 epochs\n",
        "    T_mult=2,\n",
        "    eta_min=1e-6\n",
        ")\n",
        "\n",
        "criterion = ContrastativeFocalLoss(\n",
        "    alpha=0.25,\n",
        "    gamma=2.0,\n",
        "    contrast_weight=0.3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ciclo di Addestramento e Valutazione\n",
        "\n",
        "Questa sezione finale contiene il cuore pulsante del progetto: le funzioni che definiscono il ciclo di addestramento e valutazione del modello. Il codice Ã¨ strutturato in tre parti principali per garantire chiarezza e modularitÃ .\n",
        "\n",
        "#### Funzione di Addestramento per Epoca (`train_one_epoch`)\n",
        "Questa funzione incapsula tutta la logica necessaria per eseguire una singola epoca di addestramento. Le sue responsabilitÃ  includono:\n",
        "-   **Iterazione sui Dati**: Scorre attraverso i batch forniti dal `train_loader`.\n",
        "-   **Passaggio Forward e Calcolo della Loss**: Esegue il passaggio forward del modello per ottenere sia i *logit* sia le *feature*, che vengono poi usati per calcolare la nostra `ContrastativeFocalLoss` ibrida.\n",
        "-   **Mixed Precision Training**: Per accelerare l'addestramento e ridurre l'uso di memoria della GPU, viene utilizzata la precisione mista (`torch.autocast`). Il `GradScaler` gestisce in modo sicuro il calcolo dei gradienti per prevenire problemi di underflow numerico.\n",
        "-   **Backpropagation e Ottimizzazione**: Calcola i gradienti, applica il **Gradient Clipping** (con `max_norm=1.0`) per prevenire gradienti esplosivi e stabilizzare l'addestramento, e infine aggiorna i pesi del modello.\n",
        "-   **Calcolo delle Metriche**: Restituisce la loss media e l'accuracy per l'epoca di addestramento.\n",
        "\n",
        "#### Funzione di Valutazione (`testing`)\n",
        "Questa funzione Ã¨ dedicata a valutare le performance del modello sul set di test (o validazione) al termine di ogni epoca. Ãˆ decorata con `@torch.no_grad()` per disattivare il calcolo dei gradienti, rendendo l'inferenza piÃ¹ veloce e sicura.\n",
        "-   **ModalitÃ  di Valutazione**: Imposta il modello in `model.eval()` per disattivare strati come Dropout e BatchNorm.\n",
        "-   **Calcolo di Metriche Complete**: Oltre alla loss e all'accuracy, calcola un set completo di metriche di classificazione utilizzando `scikit-learn`:\n",
        "    -   **Precision, Recall, F1-Score**: Per una valutazione approfondita dell'equilibrio del classificatore.\n",
        "    -   **AUC (Area Under the Curve)**: Per misurare la capacitÃ  del modello di distinguere tra le classi.\n",
        "    -   **Matrice di Confusione**: Per visualizzare in dettaglio gli errori di classificazione (veri positivi, falsi negativi, ecc.).\n",
        "-   **Output Strutturato**: Restituisce un dizionario contenente tutte le metriche calcolate e l'oggetto della matrice di confusione.\n",
        "\n",
        "#### Ciclo Principale di Addestramento (`train_model`)\n",
        "Questa Ã¨ la funzione orchestratrice che esegue l'intero processo di addestramento per un numero specificato di epoche.\n",
        "-   **Inizializzazione**: Prepara il `GradScaler` e l'istanza della classe `EarlyStopping`.\n",
        "-   **Loop sulle Epoche**: Per ogni epoca, esegue in sequenza `train_one_epoch` e `testing`.\n",
        "-   **Aggiornamento dello Scheduler**: Dopo ogni epoca di valutazione, aggiorna il learning rate secondo la strategia del `CosineAnnealingWarmRestarts`.\n",
        "-   **Logging e Monitoraggio**: Stampa un riepilogo dettagliato delle performance di training e test, inclusi F1-score, AUC e il learning rate corrente. Mostra anche la matrice di confusione.\n",
        "-   **Controllo Early Stopping**: Utilizza l'**F1-score** sul set di test come metrica chiave per decidere se l'addestramento deve continuare. Se non ci sono miglioramenti per un numero di epoche pari a `patience`, il ciclo viene interrotto e il modello con le migliori performance viene conservato."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1t3JWvYsKFHr"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, criterion, optimizer, scaler, device, use_mixed_precision):\n",
        "  model.train()\n",
        "\n",
        "  total_loss = 0.0\n",
        "  correct,total = 0,0\n",
        "\n",
        "  for rgb,depth,label in tqdm(loader,desc=\"Training\",leave=False):\n",
        "    rgb, depth, label = rgb.to(device), depth.to(device), label.to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if use_mixed_precision and scaler is not None:\n",
        "        with torch.autocast(device_type='cuda'):\n",
        "            out, features = model(rgb, depth, return_features=True)\n",
        "            loss = criterion(out, features, label)\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # âœ… FIX: Calcola accuracy FUORI dal context autocast\n",
        "        with torch.no_grad():\n",
        "            # Ricalcola forward in float32 per accuracy stabile\n",
        "            out_eval = model(rgb, depth, return_features=False)\n",
        "            preds = out_eval.argmax(1)\n",
        "            correct += (preds == label).sum().item()\n",
        "    else:\n",
        "        out, features = model(rgb, depth, return_features=True)\n",
        "        loss = criterion(out, features, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accuracy calculation\n",
        "        preds = out.argmax(1)\n",
        "        correct += (preds == label).sum().item()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    total += label.size(0)\n",
        "\n",
        "  avg_loss = total_loss / len(loader)\n",
        "  accuracy = 100 * correct / total\n",
        "  return avg_loss, accuracy\n",
        "\n",
        "@torch.no_grad()\n",
        "def testing(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct, total = 0, 0\n",
        "    all_preds, all_labels, all_probs = [], [], []\n",
        "\n",
        "    for rgb, depth, label in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "        rgb, depth, label = rgb.to(device), depth.to(device), label.to(device)\n",
        "\n",
        "        out = model(rgb, depth, return_features=False)\n",
        "        loss = criterion(out, label)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        probs = F.softmax(out, dim=1)\n",
        "        preds = out.argmax(1)\n",
        "\n",
        "        correct += (preds == label).sum().item()\n",
        "        total += label.size(0)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(label.cpu().numpy())\n",
        "        # âœ… FIX: Prendi SOLO la probabilitÃ  della classe 1 (fake)\n",
        "        all_probs.extend(probs[:, 1].cpu().numpy())  # Non probs.cpu().numpy()\n",
        "\n",
        "    metrics = {\n",
        "        'loss': total_loss / len(loader),\n",
        "        'accuracy': 100 * correct / total,\n",
        "        'precision': precision_score(all_labels, all_preds, zero_division=0),\n",
        "        'recall': recall_score(all_labels, all_preds, zero_division=0),\n",
        "        'f1': f1_score(all_labels, all_preds, zero_division=0),\n",
        "        'auc': roc_auc_score(all_labels, all_probs) if len(set(all_labels)) > 1 else 0.0,\n",
        "        'predictions': all_preds,\n",
        "        'labels': all_labels\n",
        "    }\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    return metrics, cm\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler,\n",
        "                device, epochs=100, patience=15, use_mixed_precision=True, save_path='best_model.pth'):\n",
        "\n",
        "  scaler = GradScaler() if use_mixed_precision else None\n",
        "  best_loss = float('inf')\n",
        "  early_stopping = EarlyStopping()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      print(f\"\\nðŸ“Š Epoch {epoch+1}/{epochs}\")\n",
        "      print(\"-\" * 40)\n",
        "\n",
        "      train_loss,train_acc = train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, use_mixed_precision)\n",
        "      test_metrics, cm = testing(model, test_loader, criterion, device)\n",
        "\n",
        "      scheduler.step()\n",
        "\n",
        "      # Stampa metriche\n",
        "      print(f\"   Train â†’ Loss: {train_loss:.4f} | Acc: {train_acc:.2f}%\")\n",
        "      print(f\"   Test  â†’ Loss: {test_metrics['loss']:.4f} | Acc: {test_metrics['accuracy']:.2f}%\")\n",
        "      print(f\"   Metrics â†’ P: {test_metrics['precision']:.3f} | R: {test_metrics['recall']:.3f} | F1: {test_metrics['f1']:.3f} | AUC: {test_metrics['auc']:.3f}\")\n",
        "      print(f\"   LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "      print(f\"\\nConfusion matrix: \\n{cm}\\n\")\n",
        "\n",
        "      # Early stopping (usa F1 come metrica principale)\n",
        "      should_stop = early_stopping(\n",
        "          test_metrics['f1'],\n",
        "          model,\n",
        "          optimizer,\n",
        "          epoch,\n",
        "          test_acc=test_metrics['accuracy'],\n",
        "          f1=test_metrics['f1']\n",
        "      )\n",
        "\n",
        "      if not should_stop:\n",
        "          print(f\"   â³ No improvement: {early_stopping.counter}/{patience}\")\n",
        "\n",
        "      if should_stop:\n",
        "          print(f\"\\nâš ï¸ Early stopping triggered at epoch {epoch+1}\")\n",
        "          break\n",
        "\n",
        "  print(f\"\\n{'='*60}\")\n",
        "  print(f\"âœ… Training completato!\")\n",
        "  print(f\"{'='*60}\\n\")\n",
        "\n",
        "  return early_stopping.best_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6nckJ2YPr2a",
        "outputId": "05666e77-b957-48f8-8800-82db27d2b9c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“Š Epoch 1/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 1.0576 | Acc: 51.41%\n",
            "   Test  â†’ Loss: 0.0571 | Acc: 29.18%\n",
            "   Metrics â†’ P: 0.755 | R: 0.094 | F1: 0.166 | AUC: 0.506\n",
            "   LR: 0.000098\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 366   38]\n",
            " [1134  117]]\n",
            "\n",
            "   ðŸ’¾ Checkpoint salvato! Score: 0.1664\n",
            "   â³ No improvement: 0/15\n",
            "\n",
            "ðŸ“Š Epoch 2/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 1.0512 | Acc: 52.53%\n",
            "   Test  â†’ Loss: 0.0528 | Acc: 33.53%\n",
            "   Metrics â†’ P: 0.952 | R: 0.127 | F1: 0.224 | AUC: 0.721\n",
            "   LR: 0.000091\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 396    8]\n",
            " [1092  159]]\n",
            "\n",
            "   ðŸ’¾ Checkpoint salvato! Score: 0.2243\n",
            "   â³ No improvement: 0/15\n",
            "\n",
            "ðŸ“Š Epoch 3/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 1.0499 | Acc: 54.44%\n",
            "   Test  â†’ Loss: 0.0459 | Acc: 49.97%\n",
            "   Metrics â†’ P: 0.988 | R: 0.342 | F1: 0.508 | AUC: 0.829\n",
            "   LR: 0.000080\n",
            "\n",
            "Confusion matrix: \n",
            "[[399   5]\n",
            " [823 428]]\n",
            "\n",
            "   ðŸ’¾ Checkpoint salvato! Score: 0.5083\n",
            "   â³ No improvement: 0/15\n",
            "\n",
            "ðŸ“Š Epoch 4/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 1.0468 | Acc: 56.40%\n",
            "   Test  â†’ Loss: 0.0484 | Acc: 49.55%\n",
            "   Metrics â†’ P: 0.986 | R: 0.337 | F1: 0.503 | AUC: 0.887\n",
            "   LR: 0.000066\n",
            "\n",
            "Confusion matrix: \n",
            "[[398   6]\n",
            " [829 422]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 1/15)\n",
            "   â³ No improvement: 1/15\n",
            "\n",
            "ðŸ“Š Epoch 5/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 1.0451 | Acc: 58.89%\n",
            "   Test  â†’ Loss: 0.0401 | Acc: 65.74%\n",
            "   Metrics â†’ P: 0.997 | R: 0.548 | F1: 0.708 | AUC: 0.932\n",
            "   LR: 0.000051\n",
            "\n",
            "Confusion matrix: \n",
            "[[402   2]\n",
            " [565 686]]\n",
            "\n",
            "   ðŸ’¾ Checkpoint salvato! Score: 0.7076\n",
            "   â³ No improvement: 0/15\n",
            "\n",
            "ðŸ“Š Epoch 6/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 1.0410 | Acc: 63.54%\n",
            "   Test  â†’ Loss: 0.0523 | Acc: 56.07%\n",
            "   Metrics â†’ P: 1.000 | R: 0.419 | F1: 0.590 | AUC: 0.942\n",
            "   LR: 0.000035\n",
            "\n",
            "Confusion matrix: \n",
            "[[404   0]\n",
            " [727 524]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 1/15)\n",
            "   â³ No improvement: 1/15\n",
            "\n",
            "ðŸ“Š Epoch 7/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 1.0386 | Acc: 63.04%\n",
            "   Test  â†’ Loss: 0.0424 | Acc: 66.77%\n",
            "   Metrics â†’ P: 1.000 | R: 0.560 | F1: 0.718 | AUC: 0.955\n",
            "   LR: 0.000021\n",
            "\n",
            "Confusion matrix: \n",
            "[[404   0]\n",
            " [550 701]]\n",
            "\n",
            "   ðŸ’¾ Checkpoint salvato! Score: 0.7182\n",
            "   â³ No improvement: 0/15\n",
            "\n",
            "ðŸ“Š Epoch 8/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 1.0373 | Acc: 64.45%\n",
            "   Test  â†’ Loss: 0.0523 | Acc: 61.27%\n",
            "   Metrics â†’ P: 1.000 | R: 0.488 | F1: 0.656 | AUC: 0.946\n",
            "   LR: 0.000010\n",
            "\n",
            "Confusion matrix: \n",
            "[[404   0]\n",
            " [641 610]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 1/15)\n",
            "   â³ No improvement: 1/15\n",
            "\n",
            "ðŸ“Š Epoch 9/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 1.0350 | Acc: 66.32%\n",
            "   Test  â†’ Loss: 0.0318 | Acc: 77.34%\n",
            "   Metrics â†’ P: 1.000 | R: 0.700 | F1: 0.824 | AUC: 0.967\n",
            "   LR: 0.000003\n",
            "\n",
            "Confusion matrix: \n",
            "[[404   0]\n",
            " [375 876]]\n",
            "\n",
            "   ðŸ’¾ Checkpoint salvato! Score: 0.8237\n",
            "   â³ No improvement: 0/15\n",
            "\n",
            "ðŸ“Š Epoch 10/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 1.0345 | Acc: 69.14%\n",
            "   Test  â†’ Loss: 0.0479 | Acc: 64.41%\n",
            "   Metrics â†’ P: 1.000 | R: 0.529 | F1: 0.692 | AUC: 0.956\n",
            "   LR: 0.000100\n",
            "\n",
            "Confusion matrix: \n",
            "[[404   0]\n",
            " [589 662]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 1/15)\n",
            "   â³ No improvement: 1/15\n",
            "\n",
            "ðŸ“Š Epoch 11/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 1.0326 | Acc: 69.81%\n",
            "   Test  â†’ Loss: 0.0309 | Acc: 78.55%\n",
            "   Metrics â†’ P: 1.000 | R: 0.716 | F1: 0.835 | AUC: 0.978\n",
            "   LR: 0.000099\n",
            "\n",
            "Confusion matrix: \n",
            "[[404   0]\n",
            " [355 896]]\n",
            "\n",
            "   ðŸ’¾ Checkpoint salvato! Score: 0.8347\n",
            "   â³ No improvement: 0/15\n",
            "\n",
            "ðŸ“Š Epoch 12/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 1.0237 | Acc: 73.63%\n",
            "   Test  â†’ Loss: 0.0236 | Acc: 83.87%\n",
            "   Metrics â†’ P: 1.000 | R: 0.787 | F1: 0.881 | AUC: 0.990\n",
            "   LR: 0.000098\n",
            "\n",
            "Confusion matrix: \n",
            "[[404   0]\n",
            " [267 984]]\n",
            "\n",
            "   ðŸ’¾ Checkpoint salvato! Score: 0.8805\n",
            "   â³ No improvement: 0/15\n",
            "\n",
            "ðŸ“Š Epoch 13/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 1.0180 | Acc: 75.21%\n",
            "   Test  â†’ Loss: 0.0156 | Acc: 87.43%\n",
            "   Metrics â†’ P: 1.000 | R: 0.834 | F1: 0.909 | AUC: 0.998\n",
            "   LR: 0.000095\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 404    0]\n",
            " [ 208 1043]]\n",
            "\n",
            "   ðŸ’¾ Checkpoint salvato! Score: 0.9093\n",
            "   â³ No improvement: 0/15\n",
            "\n",
            "ðŸ“Š Epoch 14/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 1.0036 | Acc: 79.73%\n",
            "   Test  â†’ Loss: 0.0162 | Acc: 87.92%\n",
            "   Metrics â†’ P: 1.000 | R: 0.840 | F1: 0.913 | AUC: 0.999\n",
            "   LR: 0.000091\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 404    0]\n",
            " [ 200 1051]]\n",
            "\n",
            "   ðŸ’¾ Checkpoint salvato! Score: 0.9131\n",
            "   â³ No improvement: 0/15\n",
            "\n",
            "ðŸ“Š Epoch 15/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.9906 | Acc: 82.81%\n",
            "   Test  â†’ Loss: 0.0096 | Acc: 93.96%\n",
            "   Metrics â†’ P: 1.000 | R: 0.920 | F1: 0.958 | AUC: 1.000\n",
            "   LR: 0.000086\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 404    0]\n",
            " [ 100 1151]]\n",
            "\n",
            "   ðŸ’¾ Checkpoint salvato! Score: 0.9584\n",
            "   â³ No improvement: 0/15\n",
            "\n",
            "ðŸ“Š Epoch 16/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.9715 | Acc: 85.30%\n",
            "   Test  â†’ Loss: 0.0049 | Acc: 97.58%\n",
            "   Metrics â†’ P: 1.000 | R: 0.968 | F1: 0.984 | AUC: 1.000\n",
            "   LR: 0.000080\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 404    0]\n",
            " [  40 1211]]\n",
            "\n",
            "   ðŸ’¾ Checkpoint salvato! Score: 0.9838\n",
            "   â³ No improvement: 0/15\n",
            "\n",
            "ðŸ“Š Epoch 17/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.9541 | Acc: 87.67%\n",
            "   Test  â†’ Loss: 0.0037 | Acc: 98.49%\n",
            "   Metrics â†’ P: 0.999 | R: 0.981 | F1: 0.990 | AUC: 1.000\n",
            "   LR: 0.000073\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [  24 1227]]\n",
            "\n",
            "   ðŸ’¾ Checkpoint salvato! Score: 0.9899\n",
            "   â³ No improvement: 0/15\n",
            "\n",
            "ðŸ“Š Epoch 18/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.9470 | Acc: 88.66%\n",
            "   Test  â†’ Loss: 0.0059 | Acc: 97.16%\n",
            "   Metrics â†’ P: 1.000 | R: 0.962 | F1: 0.981 | AUC: 1.000\n",
            "   LR: 0.000066\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 404    0]\n",
            " [  47 1204]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 1/15)\n",
            "   â³ No improvement: 1/15\n",
            "\n",
            "ðŸ“Š Epoch 19/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.9327 | Acc: 90.28%\n",
            "   Test  â†’ Loss: 0.0037 | Acc: 98.55%\n",
            "   Metrics â†’ P: 0.998 | R: 0.982 | F1: 0.990 | AUC: 1.000\n",
            "   LR: 0.000058\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 402    2]\n",
            " [  22 1229]]\n",
            "\n",
            "   ðŸ’¾ Checkpoint salvato! Score: 0.9903\n",
            "   â³ No improvement: 0/15\n",
            "\n",
            "ðŸ“Š Epoch 20/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.9264 | Acc: 90.74%\n",
            "   Test  â†’ Loss: 0.0028 | Acc: 99.15%\n",
            "   Metrics â†’ P: 1.000 | R: 0.989 | F1: 0.994 | AUC: 1.000\n",
            "   LR: 0.000051\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 404    0]\n",
            " [  14 1237]]\n",
            "\n",
            "   ðŸ’¾ Checkpoint salvato! Score: 0.9944\n",
            "   â³ No improvement: 0/15\n",
            "\n",
            "ðŸ“Š Epoch 21/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.9209 | Acc: 91.07%\n",
            "   Test  â†’ Loss: 0.0022 | Acc: 99.27%\n",
            "   Metrics â†’ P: 0.998 | R: 0.992 | F1: 0.995 | AUC: 1.000\n",
            "   LR: 0.000043\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 402    2]\n",
            " [  10 1241]]\n",
            "\n",
            "   ðŸ’¾ Checkpoint salvato! Score: 0.9952\n",
            "   â³ No improvement: 0/15\n",
            "\n",
            "ðŸ“Š Epoch 22/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.9113 | Acc: 92.03%\n",
            "   Test  â†’ Loss: 0.0043 | Acc: 98.13%\n",
            "   Metrics â†’ P: 0.999 | R: 0.976 | F1: 0.987 | AUC: 1.000\n",
            "   LR: 0.000035\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [  30 1221]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 1/15)\n",
            "   â³ No improvement: 1/15\n",
            "\n",
            "ðŸ“Š Epoch 23/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.9117 | Acc: 91.40%\n",
            "   Test  â†’ Loss: 0.0026 | Acc: 99.21%\n",
            "   Metrics â†’ P: 0.999 | R: 0.990 | F1: 0.995 | AUC: 1.000\n",
            "   LR: 0.000028\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [  12 1239]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 2/15)\n",
            "   â³ No improvement: 2/15\n",
            "\n",
            "ðŸ“Š Epoch 24/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.9008 | Acc: 92.77%\n",
            "   Test  â†’ Loss: 0.0022 | Acc: 99.34%\n",
            "   Metrics â†’ P: 0.998 | R: 0.993 | F1: 0.996 | AUC: 1.000\n",
            "   LR: 0.000021\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 402    2]\n",
            " [   9 1242]]\n",
            "\n",
            "   ðŸ’¾ Checkpoint salvato! Score: 0.9956\n",
            "   â³ No improvement: 0/15\n",
            "\n",
            "ðŸ“Š Epoch 25/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8978 | Acc: 93.15%\n",
            "   Test  â†’ Loss: 0.0025 | Acc: 99.15%\n",
            "   Metrics â†’ P: 0.999 | R: 0.990 | F1: 0.994 | AUC: 1.000\n",
            "   LR: 0.000015\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [  13 1238]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 1/15)\n",
            "   â³ No improvement: 1/15\n",
            "\n",
            "ðŸ“Š Epoch 26/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8915 | Acc: 93.44%\n",
            "   Test  â†’ Loss: 0.0013 | Acc: 99.64%\n",
            "   Metrics â†’ P: 0.999 | R: 0.996 | F1: 0.998 | AUC: 1.000\n",
            "   LR: 0.000010\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [   5 1246]]\n",
            "\n",
            "   ðŸ’¾ Checkpoint salvato! Score: 0.9976\n",
            "   â³ No improvement: 0/15\n",
            "\n",
            "ðŸ“Š Epoch 27/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.9008 | Acc: 92.07%\n",
            "   Test  â†’ Loss: 0.0014 | Acc: 99.70%\n",
            "   Metrics â†’ P: 0.998 | R: 0.998 | F1: 0.998 | AUC: 1.000\n",
            "   LR: 0.000006\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 402    2]\n",
            " [   3 1248]]\n",
            "\n",
            "   ðŸ’¾ Checkpoint salvato! Score: 0.9980\n",
            "   â³ No improvement: 0/15\n",
            "\n",
            "ðŸ“Š Epoch 28/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8943 | Acc: 93.98%\n",
            "   Test  â†’ Loss: 0.0014 | Acc: 99.46%\n",
            "   Metrics â†’ P: 0.999 | R: 0.994 | F1: 0.996 | AUC: 1.000\n",
            "   LR: 0.000003\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [   8 1243]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 1/15)\n",
            "   â³ No improvement: 1/15\n",
            "\n",
            "ðŸ“Š Epoch 29/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8944 | Acc: 93.02%\n",
            "   Test  â†’ Loss: 0.0013 | Acc: 99.70%\n",
            "   Metrics â†’ P: 0.998 | R: 0.998 | F1: 0.998 | AUC: 1.000\n",
            "   LR: 0.000002\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 402    2]\n",
            " [   3 1248]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 2/15)\n",
            "   â³ No improvement: 2/15\n",
            "\n",
            "ðŸ“Š Epoch 30/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8891 | Acc: 93.36%\n",
            "   Test  â†’ Loss: 0.0018 | Acc: 99.40%\n",
            "   Metrics â†’ P: 0.999 | R: 0.993 | F1: 0.996 | AUC: 1.000\n",
            "   LR: 0.000100\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [   9 1242]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 3/15)\n",
            "   â³ No improvement: 3/15\n",
            "\n",
            "ðŸ“Š Epoch 31/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8953 | Acc: 94.06%\n",
            "   Test  â†’ Loss: 0.0020 | Acc: 99.03%\n",
            "   Metrics â†’ P: 0.999 | R: 0.988 | F1: 0.994 | AUC: 1.000\n",
            "   LR: 0.000100\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [  15 1236]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 4/15)\n",
            "   â³ No improvement: 4/15\n",
            "\n",
            "ðŸ“Š Epoch 32/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.9115 | Acc: 92.61%\n",
            "   Test  â†’ Loss: 0.0039 | Acc: 98.55%\n",
            "   Metrics â†’ P: 0.999 | R: 0.982 | F1: 0.990 | AUC: 1.000\n",
            "   LR: 0.000099\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [  23 1228]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 5/15)\n",
            "   â³ No improvement: 5/15\n",
            "\n",
            "ðŸ“Š Epoch 33/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8960 | Acc: 94.27%\n",
            "   Test  â†’ Loss: 0.0013 | Acc: 99.46%\n",
            "   Metrics â†’ P: 0.999 | R: 0.994 | F1: 0.996 | AUC: 1.000\n",
            "   LR: 0.000099\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [   8 1243]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 6/15)\n",
            "   â³ No improvement: 6/15\n",
            "\n",
            "ðŸ“Š Epoch 34/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8917 | Acc: 94.44%\n",
            "   Test  â†’ Loss: 0.0045 | Acc: 98.19%\n",
            "   Metrics â†’ P: 0.999 | R: 0.977 | F1: 0.988 | AUC: 1.000\n",
            "   LR: 0.000098\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [  29 1222]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 7/15)\n",
            "   â³ No improvement: 7/15\n",
            "\n",
            "ðŸ“Š Epoch 35/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8893 | Acc: 94.35%\n",
            "   Test  â†’ Loss: 0.0015 | Acc: 99.70%\n",
            "   Metrics â†’ P: 0.999 | R: 0.997 | F1: 0.998 | AUC: 1.000\n",
            "   LR: 0.000096\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [   4 1247]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 8/15)\n",
            "   â³ No improvement: 8/15\n",
            "\n",
            "ðŸ“Š Epoch 36/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8943 | Acc: 93.90%\n",
            "   Test  â†’ Loss: 0.0018 | Acc: 99.34%\n",
            "   Metrics â†’ P: 0.999 | R: 0.992 | F1: 0.996 | AUC: 1.000\n",
            "   LR: 0.000095\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [  10 1241]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 9/15)\n",
            "   â³ No improvement: 9/15\n",
            "\n",
            "ðŸ“Š Epoch 37/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8791 | Acc: 95.43%\n",
            "   Test  â†’ Loss: 0.0007 | Acc: 99.88%\n",
            "   Metrics â†’ P: 0.998 | R: 1.000 | F1: 0.999 | AUC: 1.000\n",
            "   LR: 0.000093\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 402    2]\n",
            " [   0 1251]]\n",
            "\n",
            "   ðŸ’¾ Checkpoint salvato! Score: 0.9992\n",
            "   â³ No improvement: 0/15\n",
            "\n",
            "ðŸ“Š Epoch 38/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8822 | Acc: 94.85%\n",
            "   Test  â†’ Loss: 0.0016 | Acc: 99.34%\n",
            "   Metrics â†’ P: 0.999 | R: 0.992 | F1: 0.996 | AUC: 1.000\n",
            "   LR: 0.000091\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [  10 1241]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 1/15)\n",
            "   â³ No improvement: 1/15\n",
            "\n",
            "ðŸ“Š Epoch 39/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8790 | Acc: 95.14%\n",
            "   Test  â†’ Loss: 0.0006 | Acc: 99.94%\n",
            "   Metrics â†’ P: 0.999 | R: 1.000 | F1: 1.000 | AUC: 1.000\n",
            "   LR: 0.000088\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [   0 1251]]\n",
            "\n",
            "   ðŸ’¾ Checkpoint salvato! Score: 0.9996\n",
            "   â³ No improvement: 0/15\n",
            "\n",
            "ðŸ“Š Epoch 40/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8749 | Acc: 95.43%\n",
            "   Test  â†’ Loss: 0.0006 | Acc: 99.88%\n",
            "   Metrics â†’ P: 0.999 | R: 0.999 | F1: 0.999 | AUC: 1.000\n",
            "   LR: 0.000086\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [   1 1250]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 1/15)\n",
            "   â³ No improvement: 1/15\n",
            "\n",
            "ðŸ“Š Epoch 41/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8751 | Acc: 96.01%\n",
            "   Test  â†’ Loss: 0.0010 | Acc: 99.58%\n",
            "   Metrics â†’ P: 0.999 | R: 0.995 | F1: 0.997 | AUC: 1.000\n",
            "   LR: 0.000083\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [   6 1245]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 2/15)\n",
            "   â³ No improvement: 2/15\n",
            "\n",
            "ðŸ“Š Epoch 42/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8776 | Acc: 96.30%\n",
            "   Test  â†’ Loss: 0.0010 | Acc: 99.70%\n",
            "   Metrics â†’ P: 0.999 | R: 0.997 | F1: 0.998 | AUC: 1.000\n",
            "   LR: 0.000080\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [   4 1247]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 3/15)\n",
            "   â³ No improvement: 3/15\n",
            "\n",
            "ðŸ“Š Epoch 43/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8811 | Acc: 96.10%\n",
            "   Test  â†’ Loss: 0.0004 | Acc: 99.94%\n",
            "   Metrics â†’ P: 0.999 | R: 1.000 | F1: 1.000 | AUC: 1.000\n",
            "   LR: 0.000076\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [   0 1251]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 4/15)\n",
            "   â³ No improvement: 4/15\n",
            "\n",
            "ðŸ“Š Epoch 44/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8886 | Acc: 95.02%\n",
            "   Test  â†’ Loss: 0.0013 | Acc: 99.40%\n",
            "   Metrics â†’ P: 0.999 | R: 0.993 | F1: 0.996 | AUC: 1.000\n",
            "   LR: 0.000073\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [   9 1242]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 5/15)\n",
            "   â³ No improvement: 5/15\n",
            "\n",
            "ðŸ“Š Epoch 45/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8677 | Acc: 96.55%\n",
            "   Test  â†’ Loss: 0.0030 | Acc: 98.31%\n",
            "   Metrics â†’ P: 0.999 | R: 0.978 | F1: 0.989 | AUC: 1.000\n",
            "   LR: 0.000069\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [  27 1224]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 6/15)\n",
            "   â³ No improvement: 6/15\n",
            "\n",
            "ðŸ“Š Epoch 46/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8632 | Acc: 97.05%\n",
            "   Test  â†’ Loss: 0.0005 | Acc: 99.64%\n",
            "   Metrics â†’ P: 0.999 | R: 0.996 | F1: 0.998 | AUC: 1.000\n",
            "   LR: 0.000066\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [   5 1246]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 7/15)\n",
            "   â³ No improvement: 7/15\n",
            "\n",
            "ðŸ“Š Epoch 47/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8673 | Acc: 97.30%\n",
            "   Test  â†’ Loss: 0.0028 | Acc: 98.85%\n",
            "   Metrics â†’ P: 0.999 | R: 0.986 | F1: 0.992 | AUC: 1.000\n",
            "   LR: 0.000062\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [  18 1233]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 8/15)\n",
            "   â³ No improvement: 8/15\n",
            "\n",
            "ðŸ“Š Epoch 48/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8714 | Acc: 96.55%\n",
            "   Test  â†’ Loss: 0.0008 | Acc: 99.58%\n",
            "   Metrics â†’ P: 0.999 | R: 0.995 | F1: 0.997 | AUC: 1.000\n",
            "   LR: 0.000058\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [   6 1245]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 9/15)\n",
            "   â³ No improvement: 9/15\n",
            "\n",
            "ðŸ“Š Epoch 49/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8711 | Acc: 96.55%\n",
            "   Test  â†’ Loss: 0.0010 | Acc: 99.46%\n",
            "   Metrics â†’ P: 0.999 | R: 0.994 | F1: 0.996 | AUC: 1.000\n",
            "   LR: 0.000054\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [   8 1243]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 10/15)\n",
            "   â³ No improvement: 10/15\n",
            "\n",
            "ðŸ“Š Epoch 50/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8670 | Acc: 96.64%\n",
            "   Test  â†’ Loss: 0.0004 | Acc: 99.88%\n",
            "   Metrics â†’ P: 0.999 | R: 0.999 | F1: 0.999 | AUC: 1.000\n",
            "   LR: 0.000051\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [   1 1250]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 11/15)\n",
            "   â³ No improvement: 11/15\n",
            "\n",
            "ðŸ“Š Epoch 51/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8685 | Acc: 96.59%\n",
            "   Test  â†’ Loss: 0.0007 | Acc: 99.82%\n",
            "   Metrics â†’ P: 0.998 | R: 0.999 | F1: 0.999 | AUC: 1.000\n",
            "   LR: 0.000047\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 402    2]\n",
            " [   1 1250]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 12/15)\n",
            "   â³ No improvement: 12/15\n",
            "\n",
            "ðŸ“Š Epoch 52/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8636 | Acc: 97.18%\n",
            "   Test  â†’ Loss: 0.0006 | Acc: 99.82%\n",
            "   Metrics â†’ P: 0.999 | R: 0.998 | F1: 0.999 | AUC: 1.000\n",
            "   LR: 0.000043\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [   2 1249]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 13/15)\n",
            "   â³ No improvement: 13/15\n",
            "\n",
            "ðŸ“Š Epoch 53/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8675 | Acc: 96.68%\n",
            "   Test  â†’ Loss: 0.0004 | Acc: 99.94%\n",
            "   Metrics â†’ P: 0.999 | R: 1.000 | F1: 1.000 | AUC: 1.000\n",
            "   LR: 0.000039\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [   0 1251]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 14/15)\n",
            "   â³ No improvement: 14/15\n",
            "\n",
            "ðŸ“Š Epoch 54/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Train â†’ Loss: 0.8625 | Acc: 96.93%\n",
            "   Test  â†’ Loss: 0.0006 | Acc: 99.88%\n",
            "   Metrics â†’ P: 0.999 | R: 0.999 | F1: 0.999 | AUC: 1.000\n",
            "   LR: 0.000035\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 403    1]\n",
            " [   1 1250]]\n",
            "\n",
            "   â³ Nessun miglioramento (counter 15/15)\n",
            "\n",
            "âš ï¸ Early stopping triggered at epoch 54\n",
            "\n",
            "============================================================\n",
            "âœ… Training completato!\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "best_f1 = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    test_loader=test_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    device=device,\n",
        "    epochs=100,\n",
        "    patience=15,\n",
        "    use_mixed_precision=use_mixed_precision,\n",
        "    save_path=\"best_antispoofing_improved.pth\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rieux",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
